{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codingcat101/TeamAnM_GameSense_AdobeGenSolve_GFG/blob/main/GameSense_AnM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GameSense**\n"
      ],
      "metadata": {
        "id": "JfbuA_L2yG1h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mZBFY4O17pRg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Final Project for the AdobeGenSolve Hackathon\n",
        "\n",
        "\n",
        "\n",
        "Created by : Aarchi Kothari & Mukul J (IIT Roorkee)\n",
        "\n",
        "Crafted with ❤️ for the game of Tennis"
      ],
      "metadata": {
        "id": "mEpmCA_mvYFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abstract**\n",
        "\n",
        "Our project aims to provide automated sports insights using computer vision\n",
        "and AI/ML algorithms. Centered around two-player sport tennis, the project aims to achieve several key objectives: tracking player movements,detecting pivotal game events, automating score-keeping, and providing detailed\n",
        "metrics such as rally lengths and player activity.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xlqsmv41xyL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to run**\n",
        "\n",
        "Prepare a video file with resolution 1280x720\n",
        "\n",
        "Recommended to run on GPU"
      ],
      "metadata": {
        "id": "zRWcLrjCWLEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "beKlvtYMEK1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Importing Requirements**"
      ],
      "metadata": {
        "id": "ZB8pDSA12_g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n"
      ],
      "metadata": {
        "id": "URC2lnQG-_Vs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c711f98-c4d6-4519-f064-cd353056be8d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.7)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.1.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import catboost as ctb\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn # provides tools to build and train neural networks\n",
        "from scipy.spatial import distance\n",
        "from tqdm import tqdm\n",
        "from scipy.interpolate import CubicSpline\n",
        "from sympy import Line\n",
        "from sympy.geometry.point import Point2D\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPbp5AQxiFE6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "l8Dars8_ENgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Ball Tracking**: Accurately track the location/speed/trajectory of the shuttlecock\n",
        "or ball throughout the game."
      ],
      "metadata": {
        "id": "D1ECiNjL3dn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. **TrackNet Implementation for Tennis Ball Tracking**\n",
        "\n",
        "TrackNet: A Deep Learning Network for Tracking High-speed and Tiny Objects in Sports Applications\n",
        "\n",
        "Reference : https://arxiv.org/abs/1907.03698"
      ],
      "metadata": {
        "id": "3MuYuGjC7RUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a Convolutional Block that consists of a Conv2D layer, ReLU activation, and Batch Normalization\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, pad=1, stride=1, bias=True):\n",
        "        \"\"\"\n",
        "        Initialize a convolutional block with Conv2D, ReLU, and BatchNorm.\n",
        "\n",
        "        Parameters:\n",
        "        - in_channels: Number of input channels (e.g., 3 for RGB images)\n",
        "        - out_channels: Number of output channels (filters in the Conv2D layer)\n",
        "        - kernel_size: Size of the convolution kernel (default is 3)\n",
        "        - pad: Padding applied to the input (default is 1 to preserve spatial dimensions)\n",
        "        - stride: Stride of the convolution (default is 1 for regular convolution)\n",
        "        - bias: Whether to include a bias term in the Conv2D layer (default is True)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=pad, bias=bias),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(out_channels)  # Batch normalization to stabilize training\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the ConvBlock.\n",
        "\n",
        "        Parameters:\n",
        "        - x: Input tensor\n",
        "\n",
        "        Returns:\n",
        "        - Output tensor after passing through the Conv2D, ReLU, and BatchNorm layers\n",
        "        \"\"\"\n",
        "        return self.block(x)\n",
        "\n",
        "# Define a deep neural network for ball tracking\n",
        "class BallTrackerNet(nn.Module):\n",
        "    def __init__(self, input_channels=3, out_channels=14):\n",
        "        \"\"\"\n",
        "        Initialize the ball tracking network with a series of ConvBlocks, MaxPooling, and Upsampling layers.\n",
        "\n",
        "        Parameters:\n",
        "        - input_channels: Number of input channels (default is 3, e.g., RGB image)\n",
        "        - out_channels: Number of output channels (e.g., for detecting multiple key points)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.out_channels = out_channels\n",
        "        self.input_channels = input_channels\n",
        "\n",
        "        # Define the convolutional layers of the network\n",
        "        self.conv1 = ConvBlock(in_channels=self.input_channels, out_channels=64)\n",
        "        self.conv2 = ConvBlock(in_channels=64, out_channels=64)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # Downsample the spatial dimensions by 2\n",
        "        self.conv3 = ConvBlock(in_channels=64, out_channels=128)\n",
        "        self.conv4 = ConvBlock(in_channels=128, out_channels=128)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv5 = ConvBlock(in_channels=128, out_channels=256)\n",
        "        self.conv6 = ConvBlock(in_channels=256, out_channels=256)\n",
        "        self.conv7 = ConvBlock(in_channels=256, out_channels=256)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv8 = ConvBlock(in_channels=256, out_channels=512)\n",
        "        self.conv9 = ConvBlock(in_channels=512, out_channels=512)\n",
        "        self.conv10 = ConvBlock(in_channels=512, out_channels=512)\n",
        "\n",
        "        # Define the upsampling layers for increasing spatial resolution\n",
        "        self.ups1 = nn.Upsample(scale_factor=2)  # Upsample by a factor of 2\n",
        "        self.conv11 = ConvBlock(in_channels=512, out_channels=256)\n",
        "        self.conv12 = ConvBlock(in_channels=256, out_channels=256)\n",
        "        self.conv13 = ConvBlock(in_channels=256, out_channels=256)\n",
        "        self.ups2 = nn.Upsample(scale_factor=2)\n",
        "        self.conv14 = ConvBlock(in_channels=256, out_channels=128)\n",
        "        self.conv15 = ConvBlock(in_channels=128, out_channels=128)\n",
        "        self.ups3 = nn.Upsample(scale_factor=2)\n",
        "        self.conv16 = ConvBlock(in_channels=128, out_channels=64)\n",
        "        self.conv17 = ConvBlock(in_channels=64, out_channels=64)\n",
        "        self.conv18 = ConvBlock(in_channels=64, out_channels=self.out_channels)  # Final layer\n",
        "\n",
        "        self._init_weights()  # Initialize the weights of the network\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the BallTrackerNet.\n",
        "\n",
        "        Parameters:\n",
        "        - x: Input tensor (e.g., image or video frame)\n",
        "\n",
        "        Returns:\n",
        "        - Output tensor with the same spatial resolution as the input (after upsampling)\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.conv8(x)\n",
        "        x = self.conv9(x)\n",
        "        x = self.conv10(x)\n",
        "        x = self.ups1(x)\n",
        "        x = self.conv11(x)\n",
        "        x = self.conv12(x)\n",
        "        x = self.conv13(x)\n",
        "        x = self.ups2(x)\n",
        "        x = self.conv14(x)\n",
        "        x = self.conv15(x)\n",
        "        x = self.ups3(x)\n",
        "        x = self.conv16(x)\n",
        "        x = self.conv17(x)\n",
        "        x = self.conv18(x)\n",
        "        return x\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"\n",
        "        Initialize weights of the network. Conv2D layers are initialized with uniform distribution.\n",
        "        BatchNorm layers are initialized with constants.\n",
        "        \"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                nn.init.uniform_(module.weight, -0.05, 0.05)  # Uniform initialization\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "\n",
        "            elif isinstance(module, nn.BatchNorm2d):\n",
        "                nn.init.constant_(module.weight, 1)  # Initialize BatchNorm weight to 1\n",
        "                nn.init.constant_(module.bias, 0)   # Initialize BatchNorm bias to 0\n"
      ],
      "metadata": {
        "id": "1W8892LfB1cw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. **Ball Detection using TrackNet Trained Model**"
      ],
      "metadata": {
        "id": "nBTrAk7X-Xw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class BallDetector:\n",
        "    \"\"\"\n",
        "    A class to perform ball detection using a pretrained model.\n",
        "\n",
        "    Attributes:\n",
        "        model : BallTrackerNet\n",
        "            A deep learning model for ball tracking.\n",
        "        device : str\n",
        "            The device to run the model on ('cuda' or 'cpu').\n",
        "        width : int\n",
        "            Width of the resized frame for inference.\n",
        "        height : int\n",
        "            Height of the resized frame for inference.\n",
        "\n",
        "    Methods:\n",
        "        infer_model(frames):\n",
        "            Run the pretrained model on consecutive frames to detect ball positions.\n",
        "        postprocess(feature_map, prev_pred, scale=2, max_dist=80):\n",
        "            Process the model output to extract ball coordinates from feature maps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path_model=None, device='cuda'):\n",
        "        \"\"\"\n",
        "        Initializes the BallDetector class by loading a pretrained model if a path is provided.\n",
        "\n",
        "        Parameters:\n",
        "            path_model : str, optional\n",
        "                Path to the pretrained model weights.\n",
        "            device : str, optional\n",
        "                The device for running the model ('cuda' or 'cpu').\n",
        "        \"\"\"\n",
        "        self.model = BallTrackerNet(input_channels=9, out_channels=256)  # Initialize the ball tracking model\n",
        "        self.device = device  # Set the device (cuda or cpu)\n",
        "\n",
        "        # Load model weights if path is provided\n",
        "        if path_model:\n",
        "            self.model.load_state_dict(torch.load(path_model, map_location=device, weights_only=True))\n",
        "            self.model = self.model.to(device)  # Move the model to the specified device\n",
        "            self.model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "        # Set default frame dimensions for resizing\n",
        "        self.width = 640\n",
        "        self.height = 360\n",
        "\n",
        "    def infer_model(self, frames):\n",
        "        \"\"\"\n",
        "        Run the pretrained model on a consecutive list of frames.\n",
        "\n",
        "        Parameters:\n",
        "            frames : list\n",
        "                List of consecutive video frames.\n",
        "\n",
        "        Returns:\n",
        "            ball_track : list\n",
        "                List of detected ball positions as (x, y) coordinates for each frame.\n",
        "        \"\"\"\n",
        "        ball_track = [(None, None)]*2  # Initialize tracking result with placeholders for first two frames\n",
        "        prev_pred = [None, None]  # Store previous prediction\n",
        "\n",
        "        # Loop through the list of frames starting from the 3rd frame\n",
        "        for num in tqdm(range(2, len(frames))):\n",
        "            # Resize the current, previous, and two-frame prior frames\n",
        "            img = cv2.resize(frames[num], (self.width, self.height))\n",
        "            img_prev = cv2.resize(frames[num-1], (self.width, self.height))\n",
        "            img_preprev = cv2.resize(frames[num-2], (self.width, self.height))\n",
        "\n",
        "            # Concatenate frames to form a single input\n",
        "            imgs = np.concatenate((img, img_prev, img_preprev), axis=2)\n",
        "            imgs = imgs.astype(np.float32)/255.0  # Normalize pixel values to [0,1]\n",
        "            imgs = np.rollaxis(imgs, 2, 0)  # Change the axes order for the model input format\n",
        "\n",
        "            # Prepare the input tensor\n",
        "            inp = np.expand_dims(imgs, axis=0)\n",
        "\n",
        "            # Pass input through the model and get the output\n",
        "            out = self.model(torch.from_numpy(inp).float().to(self.device))\n",
        "            output = out.argmax(dim=1).detach().cpu().numpy()  # Get the class prediction from model output\n",
        "\n",
        "            # Postprocess the output to obtain the predicted ball position\n",
        "            x_pred, y_pred = self.postprocess(output, prev_pred)\n",
        "            prev_pred = [x_pred, y_pred]  # Update previous prediction\n",
        "            ball_track.append((x_pred, y_pred))  # Store the result\n",
        "\n",
        "        return ball_track  # Return list of detected ball positions\n",
        "\n",
        "    def postprocess(self, feature_map, prev_pred, scale=2, max_dist=80):\n",
        "        \"\"\"\n",
        "        Postprocess the output feature map from the model to get ball coordinates.\n",
        "\n",
        "        Parameters:\n",
        "            feature_map : np.array\n",
        "                Output feature map from the model with shape (1,360,640).\n",
        "            prev_pred : list\n",
        "                Previous ball prediction [x, y].\n",
        "            scale : int, optional\n",
        "                Scale factor to convert to original frame size (default is 2).\n",
        "            max_dist : int, optional\n",
        "                Maximum allowable distance from previous detection to remove outliers (default is 80).\n",
        "\n",
        "        Returns:\n",
        "            x, y : float\n",
        "                The coordinates of the detected ball position in the frame.\n",
        "        \"\"\"\n",
        "        feature_map *= 255  # Scale feature map to [0, 255] for binary thresholding\n",
        "        feature_map = feature_map.reshape((self.height, self.width))  # Reshape to original height and width\n",
        "        feature_map = feature_map.astype(np.uint8)  # Convert to unsigned 8-bit integer\n",
        "\n",
        "        # Apply binary thresholding to isolate potential ball regions\n",
        "        ret, heatmap = cv2.threshold(feature_map, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        # Detect circles using Hough Transform\n",
        "        circles = cv2.HoughCircles(heatmap, cv2.HOUGH_GRADIENT, dp=1, minDist=1, param1=50, param2=2, minRadius=2, maxRadius=7)\n",
        "        x, y = None, None\n",
        "\n",
        "        # If any circles are detected\n",
        "        if circles is not None:\n",
        "            # If there is a previous prediction\n",
        "            if prev_pred[0]:\n",
        "                # Iterate over detected circles and select one close to the previous prediction\n",
        "                for i in range(len(circles[0])):\n",
        "                    x_temp = circles[0][i][0] * scale  # Scale detected x coordinate\n",
        "                    y_temp = circles[0][i][1] * scale  # Scale detected y coordinate\n",
        "                    dist = distance.euclidean((x_temp, y_temp), prev_pred)  # Calculate distance from previous prediction\n",
        "\n",
        "                    # If the distance is within the allowed range, update the prediction\n",
        "                    if dist < max_dist:\n",
        "                        x, y = x_temp, y_temp\n",
        "                        break\n",
        "            else:\n",
        "                # If no previous prediction, take the first detected circle\n",
        "                x = circles[0][0][0] * scale\n",
        "                y = circles[0][0][1] * scale\n",
        "\n",
        "        return x, y  # Return the detected coordinates\n"
      ],
      "metadata": {
        "id": "sggSiA_-DAek"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "av4toCheEhrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  **Bounce Detection with CatBoost**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_mUjzeFMBBY9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OSgoPk0wBI9p"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class BounceDetector:\n",
        "    \"\"\"\n",
        "    A class to detect ball bounces in a sequence of frames using a CatBoost regression model.\n",
        "\n",
        "    Attributes:\n",
        "        model : CatBoostRegressor\n",
        "            A CatBoost regression model for detecting bounces.\n",
        "        threshold : float\n",
        "            A threshold value for classifying a bounce.\n",
        "\n",
        "    Methods:\n",
        "        load_model(path_model):\n",
        "            Load a pretrained CatBoost model from a file.\n",
        "        prepare_features(x_ball, y_ball):\n",
        "            Prepare the input features from the ball's x and y coordinates.\n",
        "        predict(x_ball, y_ball, smooth=True):\n",
        "            Predict bounces using the model, with an optional smoothing step.\n",
        "        smooth_predictions(x_ball, y_ball):\n",
        "            Apply smoothing to the ball's position coordinates to fill missing values.\n",
        "        extrapolate(x_coords, y_coords):\n",
        "            Extrapolate missing ball coordinates using cubic spline interpolation.\n",
        "        postprocess(ind_bounce, preds):\n",
        "            Post-process the bounce predictions to filter consecutive bounce detections.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path_model=None):\n",
        "        \"\"\"\n",
        "        Initializes the BounceDetector class and loads a model if a path is provided.\n",
        "\n",
        "        Parameters:\n",
        "            path_model : str, optional\n",
        "                Path to the pretrained CatBoost model file.\n",
        "        \"\"\"\n",
        "        self.model = ctb.CatBoostRegressor()  # Initialize CatBoost regressor\n",
        "        self.threshold = 0.45  # Set bounce prediction threshold\n",
        "\n",
        "        if path_model:\n",
        "            self.load_model(path_model)  # Load model if the path is provided\n",
        "\n",
        "    def load_model(self, path_model):\n",
        "        \"\"\"\n",
        "        Load a pretrained CatBoost model from a file.\n",
        "\n",
        "        Parameters:\n",
        "            path_model : str\n",
        "                Path to the pretrained model file.\n",
        "        \"\"\"\n",
        "        self.model.load_model(path_model)\n",
        "\n",
        "    def prepare_features(self, x_ball, y_ball):\n",
        "        \"\"\"\n",
        "        Prepare features for the model using the ball's x and y coordinates.\n",
        "\n",
        "        Parameters:\n",
        "            x_ball : list\n",
        "                List of x coordinates of the ball.\n",
        "            y_ball : list\n",
        "                List of y coordinates of the ball.\n",
        "\n",
        "        Returns:\n",
        "            features : DataFrame\n",
        "                A DataFrame containing the prepared features for prediction.\n",
        "            list of frames : list\n",
        "                List of frame numbers corresponding to the coordinates.\n",
        "        \"\"\"\n",
        "        # Create a DataFrame to store frame number, x, and y coordinates\n",
        "        labels = pd.DataFrame({'frame': range(len(x_ball)), 'x-coordinate': x_ball, 'y-coordinate': y_ball})\n",
        "\n",
        "        # Generate lagged features to capture previous and next coordinates\n",
        "        num = 3  # Number of lag frames to generate\n",
        "        eps = 1e-15  # Small constant to avoid division by zero\n",
        "        for i in range(1, num):\n",
        "            # Create lag features for x and y coordinates\n",
        "            labels['x_lag_{}'.format(i)] = labels['x-coordinate'].shift(i)\n",
        "            labels['x_lag_inv_{}'.format(i)] = labels['x-coordinate'].shift(-i)\n",
        "            labels['y_lag_{}'.format(i)] = labels['y-coordinate'].shift(i)\n",
        "            labels['y_lag_inv_{}'.format(i)] = labels['y-coordinate'].shift(-i)\n",
        "\n",
        "            # Calculate differences and ratios for lagged features\n",
        "            labels['x_diff_{}'.format(i)] = abs(labels['x_lag_{}'.format(i)] - labels['x-coordinate'])\n",
        "            labels['y_diff_{}'.format(i)] = labels['y_lag_{}'.format(i)] - labels['y-coordinate']\n",
        "            labels['x_diff_inv_{}'.format(i)] = abs(labels['x_lag_inv_{}'.format(i)] - labels['x-coordinate'])\n",
        "            labels['y_diff_inv_{}'.format(i)] = labels['y_lag_inv_{}'.format(i)] - labels['y-coordinate']\n",
        "            labels['x_div_{}'.format(i)] = abs(labels['x_diff_{}'.format(i)]/(labels['x_diff_inv_{}'.format(i)] + eps))\n",
        "            labels['y_div_{}'.format(i)] = labels['y_diff_{}'.format(i)]/(labels['y_diff_inv_{}'.format(i)] + eps)\n",
        "\n",
        "        # Remove rows with missing values due to shifting\n",
        "        for i in range(1, num):\n",
        "            labels = labels[labels['x_lag_{}'.format(i)].notna()]\n",
        "            labels = labels[labels['x_lag_inv_{}'.format(i)].notna()]\n",
        "        labels = labels[labels['x-coordinate'].notna()]\n",
        "\n",
        "        # Define feature column names for x and y coordinates\n",
        "        colnames_x = ['x_diff_{}'.format(i) for i in range(1, num)] + \\\n",
        "                     ['x_diff_inv_{}'.format(i) for i in range(1, num)] + \\\n",
        "                     ['x_div_{}'.format(i) for i in range(1, num)]\n",
        "        colnames_y = ['y_diff_{}'.format(i) for i in range(1, num)] + \\\n",
        "                     ['y_diff_inv_{}'.format(i) for i in range(1, num)] + \\\n",
        "                     ['y_div_{}'.format(i) for i in range(1, num)]\n",
        "        colnames = colnames_x + colnames_y\n",
        "\n",
        "        # Extract features for prediction\n",
        "        features = labels[colnames]\n",
        "        return features, list(labels['frame'])\n",
        "\n",
        "    def predict(self, x_ball, y_ball, smooth=True):\n",
        "        \"\"\"\n",
        "        Predict the frames where a bounce occurs using the model.\n",
        "\n",
        "        Parameters:\n",
        "            x_ball : list\n",
        "                List of x coordinates of the ball.\n",
        "            y_ball : list\n",
        "                List of y coordinates of the ball.\n",
        "            smooth : bool, optional\n",
        "                Whether to smooth the ball coordinates before prediction (default is True).\n",
        "\n",
        "        Returns:\n",
        "            set\n",
        "                A set of frame numbers where a bounce is detected.\n",
        "        \"\"\"\n",
        "        # Apply smoothing to the ball's coordinates if required\n",
        "        if smooth:\n",
        "            x_ball, y_ball = self.smooth_predictions(x_ball, y_ball)\n",
        "\n",
        "        # Prepare features for prediction\n",
        "        features, num_frames = self.prepare_features(x_ball, y_ball)\n",
        "\n",
        "        # Predict bounce probabilities using the model\n",
        "        preds = self.model.predict(features)\n",
        "\n",
        "        # Find frames where bounce probability exceeds the threshold\n",
        "        ind_bounce = np.where(preds > self.threshold)[0]\n",
        "\n",
        "        # Post-process bounce predictions to remove consecutive detections\n",
        "        if len(ind_bounce) > 0:\n",
        "            ind_bounce = self.postprocess(ind_bounce, preds)\n",
        "\n",
        "        # Get the frame numbers where bounces are predicted\n",
        "        frames_bounce = [num_frames[x] for x in ind_bounce]\n",
        "        return set(frames_bounce)\n",
        "\n",
        "    def smooth_predictions(self, x_ball, y_ball):\n",
        "        \"\"\"\n",
        "        Smooth ball coordinates by filling missing values using extrapolation.\n",
        "\n",
        "        Parameters:\n",
        "            x_ball : list\n",
        "                List of x coordinates of the ball.\n",
        "            y_ball : list\n",
        "                List of y coordinates of the ball.\n",
        "\n",
        "        Returns:\n",
        "            x_ball : list\n",
        "                Smoothed x coordinates.\n",
        "            y_ball : list\n",
        "                Smoothed y coordinates.\n",
        "        \"\"\"\n",
        "        is_none = [int(x is None) for x in x_ball]  # Flag missing values\n",
        "        interp = 5  # Number of previous frames to use for extrapolation\n",
        "        counter = 0\n",
        "\n",
        "        # Iterate through frames to smooth missing values\n",
        "        for num in range(interp, len(x_ball)-1):\n",
        "            if not x_ball[num] and sum(is_none[num-interp:num]) == 0 and counter < 3:\n",
        "                # Extrapolate missing values using previous frames\n",
        "                x_ext, y_ext = self.extrapolate(x_ball[num-interp:num], y_ball[num-interp:num])\n",
        "                x_ball[num], y_ball[num] = x_ext, y_ext  # Update with extrapolated values\n",
        "                is_none[num] = 0\n",
        "\n",
        "                # If the next value exists, check if extrapolated value is valid\n",
        "                if x_ball[num+1]:\n",
        "                    dist = distance.euclidean((x_ext, y_ext), (x_ball[num+1], y_ball[num+1]))\n",
        "                    if dist > 80:\n",
        "                        # Mark next frame as None if distance is too large\n",
        "                        x_ball[num+1], y_ball[num+1], is_none[num+1] = None, None, 1\n",
        "                counter += 1\n",
        "            else:\n",
        "                counter = 0\n",
        "        return x_ball, y_ball\n",
        "\n",
        "    def extrapolate(self, x_coords, y_coords):\n",
        "        \"\"\"\n",
        "        Extrapolate missing ball coordinates using cubic spline interpolation.\n",
        "\n",
        "        Parameters:\n",
        "            x_coords : list\n",
        "                List of x coordinates for interpolation.\n",
        "            y_coords : list\n",
        "                List of y coordinates for interpolation.\n",
        "\n",
        "        Returns:\n",
        "            x_ext : float\n",
        "                Extrapolated x coordinate.\n",
        "            y_ext : float\n",
        "                Extrapolated y coordinate.\n",
        "        \"\"\"\n",
        "        xs = list(range(len(x_coords)))  # Frame indices for the coordinates\n",
        "\n",
        "        # Apply cubic spline interpolation to x and y coordinates\n",
        "        func_x = CubicSpline(xs, x_coords, bc_type='natural')\n",
        "        func_y = CubicSpline(xs, y_coords, bc_type='natural')\n",
        "\n",
        "        # Extrapolate to the next frame\n",
        "        x_ext = func_x(len(x_coords))\n",
        "        y_ext = func_y(len(x_coords))\n",
        "\n",
        "        return float(x_ext), float(y_ext)\n",
        "\n",
        "    def postprocess(self, ind_bounce, preds):\n",
        "        \"\"\"\n",
        "        Post-process bounce predictions to filter out consecutive detections.\n",
        "\n",
        "        Parameters:\n",
        "            ind_bounce : list\n",
        "                List of indices where bounces are predicted.\n",
        "            preds : list\n",
        "                List of prediction probabilities for each frame.\n",
        "\n",
        "        Returns:\n",
        "            ind_bounce_filtered : list\n",
        "                List of filtered bounce prediction indices.\n",
        "        \"\"\"\n",
        "        ind_bounce_filtered = [ind_bounce[0]]  # Keep the first bounce\n",
        "\n",
        "        # Iterate through bounce predictions\n",
        "        for i in range(1, len(ind_bounce)):\n",
        "            if (ind_bounce[i] - ind_bounce[i-1]) != 1:\n",
        "                # Keep non-consecutive bounce predictions\n",
        "                ind_bounce_filtered.append(ind_bounce[i])\n",
        "            elif preds[ind_bounce[i]] > preds[ind_bounce[i-1]]:\n",
        "                # Keep the bounce with higher probability for consecutive frames\n",
        "                ind_bounce_filtered[-1] = ind_bounce[i]\n",
        "\n",
        "        return ind_bounce_filtered\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yHU-MJVOEpIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Play Area Marking**: Accurately identify and mark play area boundaries, including\n",
        "the net, court boundaries, and other relevant critical game areas."
      ],
      "metadata": {
        "id": "z0duEa8iC-Oq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. **Line Detection and Refinement in Images**"
      ],
      "metadata": {
        "id": "yNRN66RqC3o0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This script provides functions for detecting lines in an image, merging close lines,\n",
        "# and refining key points by finding the intersection of the detected lines.\n",
        "\n",
        "\n",
        "def line_intersection(line1, line2):\n",
        "    \"\"\"\n",
        "    Find the intersection point of two lines.\n",
        "\n",
        "    Parameters:\n",
        "    - line1: List or tuple containing four elements representing the coordinates (x1, y1, x2, y2) of the first line.\n",
        "    - line2: List or tuple containing four elements representing the coordinates (x1, y1, x2, y2) of the second line.\n",
        "\n",
        "    Returns:\n",
        "    - point: The intersection point as a tuple (x, y) if the lines intersect, otherwise None.\n",
        "    \"\"\"\n",
        "    l1 = Line((line1[0], line1[1]), (line1[2], line1[3]))\n",
        "    l2 = Line((line2[0], line2[1]), (line2[2], line2[3]))\n",
        "\n",
        "    intersection = l1.intersection(l2)\n",
        "    point = None\n",
        "    if len(intersection) > 0:\n",
        "        if isinstance(intersection[0], Point2D):\n",
        "            point = intersection[0].coordinates\n",
        "    return point\n",
        "\n",
        "def refine_kps(img, x_ct, y_ct, crop_size=40):\n",
        "    \"\"\"\n",
        "    Refines the key points of an image based on detected lines.\n",
        "\n",
        "    Parameters:\n",
        "    - img: The input image in which key points are to be refined.\n",
        "    - x_ct, y_ct: The initial x and y coordinates of the key point.\n",
        "    - crop_size: The size of the crop around the key point for line detection (default is 40).\n",
        "\n",
        "    Returns:\n",
        "    - refined_y_ct, refined_x_ct: The refined coordinates of the key point.\n",
        "    \"\"\"\n",
        "    refined_x_ct, refined_y_ct = x_ct, y_ct\n",
        "\n",
        "    img_height, img_width = img.shape[:2]\n",
        "    x_min = max(x_ct - crop_size, 0)\n",
        "    x_max = min(img_height, x_ct + crop_size)\n",
        "    y_min = max(y_ct - crop_size, 0)\n",
        "    y_max = min(img_width, y_ct + crop_size)\n",
        "\n",
        "    img_crop = img[x_min:x_max, y_min:y_max]\n",
        "    lines = detect_lines(img_crop)\n",
        "\n",
        "    if len(lines) > 1:\n",
        "        lines = merge_lines(lines)\n",
        "        if len(lines) == 2:\n",
        "            inters = line_intersection(lines[0], lines[1])\n",
        "            if inters:\n",
        "                new_x_ct = int(inters[1])\n",
        "                new_y_ct = int(inters[0])\n",
        "                if new_x_ct > 0 and new_x_ct < img_crop.shape[0] and new_y_ct > 0 and new_y_ct < img_crop.shape[1]:\n",
        "                    refined_x_ct = x_min + new_x_ct\n",
        "                    refined_y_ct = y_min + new_y_ct\n",
        "    return refined_y_ct, refined_x_ct\n",
        "\n",
        "def detect_lines(image):\n",
        "    \"\"\"\n",
        "    Detects lines in an image using the Hough Line Transform.\n",
        "\n",
        "    Parameters:\n",
        "    - image: The input image for line detection.\n",
        "\n",
        "    Returns:\n",
        "    - lines: A list of detected lines in the format [[x1, y1, x2, y2], ...].\n",
        "    \"\"\"\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.threshold(gray, 155, 255, cv2.THRESH_BINARY)[1]\n",
        "    lines = cv2.HoughLinesP(gray, 1, np.pi / 180, 30, minLineLength=10, maxLineGap=30)\n",
        "    lines = np.squeeze(lines)\n",
        "    if len(lines.shape) > 0:\n",
        "        if len(lines) == 4 and not isinstance(lines[0], np.ndarray):\n",
        "            lines = [lines]\n",
        "    else:\n",
        "        lines = []\n",
        "    return lines\n",
        "\n",
        "def merge_lines(lines):\n",
        "    \"\"\"\n",
        "    Merges lines that are close to each other based on their end points.\n",
        "\n",
        "    Parameters:\n",
        "    - lines: A list of lines in the format [[x1, y1, x2, y2], ...].\n",
        "\n",
        "    Returns:\n",
        "    - new_lines: A list of merged lines.\n",
        "    \"\"\"\n",
        "    lines = sorted(lines, key=lambda item: item[0])\n",
        "    mask = [True] * len(lines)\n",
        "    new_lines = []\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        if mask[i]:\n",
        "            for j, s_line in enumerate(lines[i + 1:]):\n",
        "                if mask[i + j + 1]:\n",
        "                    x1, y1, x2, y2 = line\n",
        "                    x3, y3, x4, y4 = s_line\n",
        "                    dist1 = distance.euclidean((x1, y1), (x3, y3))\n",
        "                    dist2 = distance.euclidean((x2, y2), (x4, y4))\n",
        "                    if dist1 < 20 and dist2 < 20:\n",
        "                        line = np.array([int((x1 + x3) / 2), int((y1 + y3) / 2), int((x2 + x4) / 2), int((y2 + y4) / 2)])\n",
        "                        mask[i + j + 1] = False\n",
        "            new_lines.append(line)\n",
        "    return new_lines\n",
        "\n"
      ],
      "metadata": {
        "id": "OcxFd1hfCUHP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Court Reference Model**"
      ],
      "metadata": {
        "id": "rtb-fl0YGKYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **Court Reference Model**\n",
        "\n",
        "# This script defines a `CourtReference` class that models a court layout,\n",
        "# providing functions to build a court reference image, get important lines,\n",
        "# save court configurations, and generate court masks.\n",
        "\n",
        "\n",
        "\n",
        "class CourtReference:\n",
        "    \"\"\"\n",
        "    A class to represent a tennis or similar sports court and provide methods to manipulate its layout.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the court reference model with court lines, key points, and configurations.\n",
        "        \"\"\"\n",
        "        self.baseline_top = ((286, 561), (1379, 561))\n",
        "        self.baseline_bottom = ((286, 2935), (1379, 2935))\n",
        "        self.net = ((286, 1748), (1379, 1748))\n",
        "        self.left_court_line = ((286, 561), (286, 2935))\n",
        "        self.right_court_line = ((1379, 561), (1379, 2935))\n",
        "        self.left_inner_line = ((423, 561), (423, 2935))\n",
        "        self.right_inner_line = ((1242, 561), (1242, 2935))\n",
        "        self.middle_line = ((832, 1110), (832, 2386))\n",
        "        self.top_inner_line = ((423, 1110), (1242, 1110))\n",
        "        self.bottom_inner_line = ((423, 2386), (1242, 2386))\n",
        "        self.top_extra_part = (832.5, 580)\n",
        "        self.bottom_extra_part = (832.5, 2910)\n",
        "\n",
        "        # Define key points and configurations\n",
        "        self.key_points = [*self.baseline_top, *self.baseline_bottom,\n",
        "                          *self.left_inner_line, *self.right_inner_line,\n",
        "                          *self.top_inner_line, *self.bottom_inner_line,\n",
        "                          *self.middle_line]\n",
        "\n",
        "        self.border_points = [*self.baseline_top, *self.baseline_bottom[::-1]]\n",
        "\n",
        "        # Configurations for different court layouts\n",
        "        self.court_conf = {1: [*self.baseline_top, *self.baseline_bottom],\n",
        "                           2: [self.left_inner_line[0], self.right_inner_line[0], self.left_inner_line[1],\n",
        "                               self.right_inner_line[1]],\n",
        "                           3: [self.left_inner_line[0], self.right_court_line[0], self.left_inner_line[1],\n",
        "                               self.right_court_line[1]],\n",
        "                           4: [self.left_court_line[0], self.right_inner_line[0], self.left_court_line[1],\n",
        "                               self.right_inner_line[1]],\n",
        "                           5: [*self.top_inner_line, *self.bottom_inner_line],\n",
        "                           6: [*self.top_inner_line, self.left_inner_line[1], self.right_inner_line[1]],\n",
        "                           7: [self.left_inner_line[0], self.right_inner_line[0], *self.bottom_inner_line],\n",
        "                           8: [self.right_inner_line[0], self.right_court_line[0], self.right_inner_line[1],\n",
        "                               self.right_court_line[1]],\n",
        "                           9: [self.left_court_line[0], self.left_inner_line[0], self.left_court_line[1],\n",
        "                               self.left_inner_line[1]],\n",
        "                           10: [self.top_inner_line[0], self.middle_line[0], self.bottom_inner_line[0],\n",
        "                                self.middle_line[1]],\n",
        "                           11: [self.middle_line[0], self.top_inner_line[1], self.middle_line[1],\n",
        "                                self.bottom_inner_line[1]],\n",
        "                           12: [*self.bottom_inner_line, self.left_inner_line[1], self.right_inner_line[1]]}\n",
        "\n",
        "        # Court dimensions\n",
        "        self.line_width = 1\n",
        "        self.court_width = 1117\n",
        "        self.court_height = 2408\n",
        "        self.top_bottom_border = 549\n",
        "        self.right_left_border = 274\n",
        "        self.court_total_width = self.court_width + self.right_left_border * 2\n",
        "        self.court_total_height = self.court_height + self.top_bottom_border * 2\n",
        "        self.court = self.build_court_reference()\n",
        "\n",
        "    def build_court_reference(self):\n",
        "        \"\"\"\n",
        "        Create an image of the court reference using the defined line positions.\n",
        "\n",
        "        Returns:\n",
        "        - court: A binary image representing the court with lines drawn.\n",
        "        \"\"\"\n",
        "        court = np.zeros((self.court_height + 2 * self.top_bottom_border, self.court_width + 2 * self.right_left_border), dtype=np.uint8)\n",
        "        cv2.line(court, *self.baseline_top, 1, self.line_width)\n",
        "        cv2.line(court, *self.baseline_bottom, 1, self.line_width)\n",
        "        cv2.line(court, *self.net, 1, self.line_width)\n",
        "        cv2.line(court, *self.top_inner_line, 1, self.line_width)\n",
        "        cv2.line(court, *self.bottom_inner_line, 1, self.line_width)\n",
        "        cv2.line(court, *self.left_court_line, 1, self.line_width)\n",
        "        cv2.line(court, *self.right_court_line, 1, self.line_width)\n",
        "        cv2.line(court, *self.left_inner_line, 1, self.line_width)\n",
        "        cv2.line(court, *self.right_inner_line, 1, self.line_width)\n",
        "        cv2.line(court, *self.middle_line, 1, self.line_width)\n",
        "        court = cv2.dilate(court, np.ones((5, 5), dtype=np.uint8))\n",
        "        # court = cv2.dilate(court, np.ones((7, 7), dtype=np.uint8))\n",
        "        # plt.imsave('court_configurations/court_reference.png', court, cmap='gray')\n",
        "        # self.court = court\n",
        "        return court\n",
        "    def get_important_lines(self):\n",
        "        \"\"\"\n",
        "        Returns all the important lines that define the court layout.\n",
        "\n",
        "        Returns:\n",
        "        - lines: A list of all court line coordinates.\n",
        "        \"\"\"\n",
        "        lines = [*self.baseline_top, *self.baseline_bottom, *self.net, *self.left_court_line, *self.right_court_line,\n",
        "                 *self.left_inner_line, *self.right_inner_line, *self.middle_line,\n",
        "                 *self.top_inner_line, *self.bottom_inner_line]\n",
        "        return lines\n",
        "\n",
        "    def get_extra_parts(self):\n",
        "        \"\"\"\n",
        "        Returns the extra parts of the court that are outside the main playing area.\n",
        "\n",
        "        Returns:\n",
        "        - parts: A list of coordinates representing extra parts of the court.\n",
        "        \"\"\"\n",
        "        parts = [self.top_extra_part, self.bottom_extra_part]\n",
        "        return parts\n",
        "\n",
        "    def save_all_court_configurations(self):\n",
        "        \"\"\"\n",
        "        Save all configurations of 4 points on the court reference image.\n",
        "\n",
        "        This function generates and saves images for different court configurations defined in `self.court_conf`.\n",
        "        \"\"\"\n",
        "        for i, conf in self.court_conf.items():\n",
        "            c = cv2.cvtColor(255 - self.court, cv2.COLOR_GRAY2BGR)\n",
        "            for p in conf:\n",
        "                c = cv2.circle(c, p, 15, (0, 0, 255), 30)  # Mark points with circles\n",
        "            cv2.imwrite(f'court_configurations/court_conf_{i}.png', c)\n",
        "\n",
        "    def get_court_mask(self, mask_type=0):\n",
        "        \"\"\"\n",
        "        Generate a mask of the court based on the specified type.\n",
        "\n",
        "        Parameters:\n",
        "        - mask_type: An integer indicating the type of mask to create:\n",
        "                     0 - Full court\n",
        "                     1 - Bottom half court\n",
        "                     2 - Top half court\n",
        "                     3 - Court without margins\n",
        "\n",
        "        Returns:\n",
        "        - mask: A binary mask of the court.\n",
        "        \"\"\"\n",
        "        mask = np.ones_like(self.court)\n",
        "        if mask_type == 1:  # Bottom half court\n",
        "            mask[:self.net[0][1], :] = 0\n",
        "        elif mask_type == 2:  # Top half court\n",
        "            mask[self.net[0][1]:, :] = 0\n",
        "        elif mask_type == 3:  # Court without margins\n",
        "            mask[:self.baseline_top[0][1], :] = 0\n",
        "            mask[self.baseline_bottom[0][1]:, :] = 0\n",
        "            mask[:, :self.left_court_line[0][0]] = 0\n",
        "            mask[:, self.right_court_line[0][0]:] = 0\n",
        "        return mask\n",
        "\n",
        "# **Usage Example**\n",
        "if __name__ == '__main__':\n",
        "    c = CourtReference()\n",
        "    c.build_court_reference()\n",
        "    # You can now call various methods on the `c` object to manipulate the court image.\n"
      ],
      "metadata": {
        "id": "NoNbmgnwCfZi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OaonVFEzVE8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Player Detection and Tracking on Court**"
      ],
      "metadata": {
        "id": "cXYlnp9cII4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the PersonDetector class\n",
        "class PersonDetector:\n",
        "    \"\"\"\n",
        "    Detects and tracks people on the sports court using a pre-trained Faster R-CNN model.\n",
        "\n",
        "    This class detects persons in images and tracks their movements over time, focusing on the top\n",
        "    and bottom halves of the court.\n",
        "    \"\"\"\n",
        "    def __init__(self, dtype=torch.FloatTensor):\n",
        "        # Load pre-trained Faster R-CNN model\n",
        "        self.detection_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "        self.detection_model = self.detection_model.to(dtype)\n",
        "        self.detection_model.eval()\n",
        "        self.dtype = dtype\n",
        "        self.court_ref = CourtReference()\n",
        "        # Obtain court masks for top and bottom halves\n",
        "        self.ref_top_court = self.court_ref.get_court_mask(2)\n",
        "        self.ref_bottom_court = self.court_ref.get_court_mask(1)\n",
        "        self.point_person_top = None\n",
        "        self.point_person_bottom = None\n",
        "        self.counter_top = 0\n",
        "        self.counter_bottom = 0\n",
        "        self.top_distances = []\n",
        "        self.bottom_distances = []\n",
        "        self.active_time_top = 0\n",
        "        self.active_time_bottom = 0\n",
        "\n",
        "    def detect(self, image, person_min_score=0.85):\n",
        "        \"\"\"\n",
        "        Detect persons in a given image using the pre-trained Faster R-CNN model.\n",
        "\n",
        "        Parameters:\n",
        "        image : np.array\n",
        "            The input image where persons need to be detected.\n",
        "        person_min_score : float\n",
        "            The minimum confidence score for considering a detection as a person.\n",
        "\n",
        "        Returns:\n",
        "        persons_boxes : list\n",
        "            List of detected bounding boxes for persons.\n",
        "        probs : list\n",
        "            List of corresponding probabilities for each detection.\n",
        "        \"\"\"\n",
        "        PERSON_LABEL = 1\n",
        "        frame_tensor = image.transpose((2, 0, 1)) / 255\n",
        "        frame_tensor = torch.from_numpy(frame_tensor).unsqueeze(0).float().to(self.dtype)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = self.detection_model(frame_tensor)\n",
        "\n",
        "        persons_boxes = []\n",
        "        probs = []\n",
        "        for box, label, score in zip(preds[0]['boxes'], preds[0]['labels'], preds[0]['scores']):\n",
        "            if label == PERSON_LABEL and score > person_min_score:\n",
        "                persons_boxes.append(box.detach().cpu().numpy())\n",
        "                probs.append(score.detach().cpu().numpy())\n",
        "        return persons_boxes, probs\n",
        "\n",
        "    def detect_top_and_bottom_players(self, image, inv_matrix, filter_players=False):\n",
        "        \"\"\"\n",
        "        Detect players on the top and bottom halves of the court.\n",
        "\n",
        "        Parameters:\n",
        "        image : np.array\n",
        "            The input image to detect players.\n",
        "        inv_matrix : np.array\n",
        "            Inverse transformation matrix for perspective correction.\n",
        "        filter_players : bool\n",
        "            Whether to filter out multiple detected players to find the key player.\n",
        "\n",
        "        Returns:\n",
        "        person_bboxes_top : list\n",
        "            Detected bounding boxes of players in the top half.\n",
        "        person_bboxes_bottom : list\n",
        "            Detected bounding boxes of players in the bottom half.\n",
        "        \"\"\"\n",
        "        matrix = cv2.invert(inv_matrix)[1]\n",
        "        mask_top_court = cv2.warpPerspective(self.ref_top_court, matrix, image.shape[1::-1])\n",
        "        mask_bottom_court = cv2.warpPerspective(self.ref_bottom_court, matrix, image.shape[1::-1])\n",
        "        person_bboxes_top, person_bboxes_bottom = [], []\n",
        "\n",
        "        bboxes, probs = self.detect(image, person_min_score=0.85)\n",
        "        if len(bboxes) > 0:\n",
        "            person_points = [[int((bbox[2] + bbox[0]) / 2), int(bbox[3])] for bbox in bboxes]\n",
        "            person_bboxes = list(zip(bboxes, person_points))\n",
        "\n",
        "            person_bboxes_top = [pt for pt in person_bboxes if mask_top_court[pt[1][1]-1, pt[1][0]] == 1]\n",
        "            person_bboxes_bottom = [pt for pt in person_bboxes if mask_bottom_court[pt[1][1] - 1, pt[1][0]] == 1]\n",
        "\n",
        "            if filter_players:\n",
        "                person_bboxes_top, person_bboxes_bottom = self.filter_players(person_bboxes_top, person_bboxes_bottom, matrix)\n",
        "\n",
        "        return person_bboxes_top, person_bboxes_bottom\n",
        "\n",
        "    def filter_players(self, person_bboxes_top, person_bboxes_bottom, matrix):\n",
        "        \"\"\"\n",
        "        Filter out multiple detected players to identify key players based on proximity to court center.\n",
        "        \"\"\"\n",
        "        players_top_pts = [player[1] for player in person_bboxes_top]\n",
        "        players_bottom_pts = [player[1] for player in person_bboxes_bottom]\n",
        "\n",
        "        if players_top_pts:\n",
        "            players_top_pts_court = cv2.perspectiveTransform(np.array([players_top_pts], dtype=np.float32), matrix)[0]\n",
        "            dist_top = distance.cdist(players_top_pts_court, [[self.court_ref.top_extra_part]])\n",
        "            person_bboxes_top = [person_bboxes_top[np.argmin(dist_top)]]\n",
        "\n",
        "        if players_bottom_pts:\n",
        "            players_bottom_pts_court = cv2.perspectiveTransform(np.array([players_bottom_pts], dtype=np.float32), matrix)[0]\n",
        "            dist_bottom = distance.cdist(players_bottom_pts_court, [[self.court_ref.bottom_extra_part]])\n",
        "            person_bboxes_bottom = [person_bboxes_bottom[np.argmin(dist_bottom)]]\n",
        "\n",
        "        return person_bboxes_top, person_bboxes_bottom\n",
        "    def track_players(self, frames, matrix_all, filter_players=False):\n",
        "        # tracks the players for the duration of the frames and returns lists of the players' coordinates\n",
        "        persons_top = []\n",
        "        persons_bottom = []\n",
        "        min_len = min(len(frames), len(matrix_all))\n",
        "        for num_frame in tqdm(range(min_len)):\n",
        "            img = frames[num_frame]\n",
        "            if matrix_all[num_frame] is not None:\n",
        "                inv_matrix = matrix_all[num_frame]\n",
        "                person_top, person_bottom = self.detect_top_and_bottom_players(img, inv_matrix, filter_players)\n",
        "            else:\n",
        "                person_top, person_bottom = [], []\n",
        "\n",
        "\n",
        "            persons_top.append(person_top)\n",
        "            persons_bottom.append(person_bottom)\n",
        "\n",
        "        return persons_top, persons_bottom\n"
      ],
      "metadata": {
        "id": "AuF0E3-_CjeO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nofFTShFVH5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Homography Matrix**"
      ],
      "metadata": {
        "id": "QMOLMDZ-QCTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the court reference\n",
        "court_ref = CourtReference()\n",
        "\n",
        "# Reshape court reference key points for perspective transformation\n",
        "refer_kps = np.array(court_ref.key_points, dtype=np.float32).reshape((-1, 1, 2))\n",
        "# Create a mapping of court configuration indices to corresponding key point indices\n",
        "court_conf_ind = {}\n",
        "for i in range(len(court_ref.court_conf)):\n",
        "    conf = court_ref.court_conf[i+1]\n",
        "    inds = []\n",
        "    for j in range(4):\n",
        "        inds.append(court_ref.key_points.index(conf[j]))\n",
        "    court_conf_ind[i+1] = inds\n",
        "\n",
        "\n",
        "def get_trans_matrix(points):\n",
        "    \"\"\"\n",
        "    Determine the best homography (transformation) matrix from the given points and court configuration.\n",
        "\n",
        "    This function computes a homography matrix based on the court's configuration and\n",
        "    compares distances to find the optimal transformation that aligns the points with the court layout.\n",
        "\n",
        "    Parameters:\n",
        "    points : list\n",
        "        List of points representing detected key points from the input image.\n",
        "\n",
        "    Returns:\n",
        "    matrix_trans : np.array or None\n",
        "        The best homography matrix for perspective transformation, or None if no valid transformation is found.\n",
        "    \"\"\"\n",
        "    matrix_trans = None\n",
        "    dist_max = np.Inf  # Initialize with a large value for distance comparison\n",
        "\n",
        "    # Iterate through court configurations to find the best transformation\n",
        "    for conf_ind in range(1, 13):  # Loop through 12 court configurations\n",
        "        conf = court_ref.court_conf[conf_ind]  # Retrieve the court configuration for this index\n",
        "        inds = court_conf_ind[conf_ind]  # Get corresponding key point indices\n",
        "\n",
        "        # Gather intersection points based on the indices\n",
        "        inters = [points[inds[0]], points[inds[1]], points[inds[2]], points[inds[3]]]\n",
        "\n",
        "        if None not in inters:  # Ensure that all intersection points are available\n",
        "            # Compute homography matrix using key points and court reference points\n",
        "            matrix, _ = cv2.findHomography(np.float32(conf), np.float32(inters), method=0)\n",
        "\n",
        "            # Apply the transformation to court reference key points\n",
        "            trans_kps = cv2.perspectiveTransform(refer_kps, matrix).squeeze(1)\n",
        "\n",
        "            # Calculate the Euclidean distances for key points not in the indices\n",
        "            dists = []\n",
        "            for i in range(12):\n",
        "                if i not in inds and points[i] is not None:\n",
        "                    dists.append(distance.euclidean(points[i], trans_kps[i]))\n",
        "\n",
        "            # Compute the mean distance to compare different configurations\n",
        "            dist_median = np.mean(dists)\n",
        "\n",
        "            # Update the transformation matrix if a better (smaller) median distance is found\n",
        "            if dist_median < dist_max:\n",
        "                matrix_trans = matrix\n",
        "                dist_max = dist_median\n",
        "\n",
        "    return matrix_trans  # Return the best transformation matrix\n"
      ],
      "metadata": {
        "id": "VmOBceSTCxbA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Court Detection using pre-trained model"
      ],
      "metadata": {
        "id": "OQ8rZQi7SD9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CourtDetectorNet:\n",
        "    \"\"\"\n",
        "    A class for detecting and tracking key points on a sports court using a neural network model.\n",
        "\n",
        "    This class uses a pre-trained neural network model to infer the positions of key points (e.g., court lines and net)\n",
        "    from a sequence of frames and estimates the transformation matrix for perspective correction.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path_model=None, device='cuda'):\n",
        "        \"\"\"\n",
        "        Initialize the CourtDetectorNet class with the provided model and device.\n",
        "\n",
        "        Parameters:\n",
        "        path_model : str or None\n",
        "            Path to the pre-trained model weights (if available). If None, a new model is initialized.\n",
        "        device : str\n",
        "            The device to run the model on ('cuda' for GPU or 'cpu' for CPU).\n",
        "        \"\"\"\n",
        "        # Initialize the BallTrackerNet model with 15 output channels (corresponding to key points)\n",
        "        self.model = BallTrackerNet(out_channels=15)\n",
        "        self.device = device\n",
        "\n",
        "        # Load pre-trained model weights if provided\n",
        "        if path_model:\n",
        "            self.model.load_state_dict(torch.load(path_model, map_location=device, weights_only=True))\n",
        "            self.model = self.model.to(device)\n",
        "            self.model.eval()\n",
        "\n",
        "    def infer_model(self, frames):\n",
        "        \"\"\"\n",
        "        Perform inference on a sequence of frames to detect court key points and transformation matrices.\n",
        "\n",
        "        Parameters:\n",
        "        frames : list of np.array\n",
        "            A list of video frames (images) to process.\n",
        "\n",
        "        Returns:\n",
        "        matrixes_res : list\n",
        "            A list of transformation matrices for each frame.\n",
        "        kps_res : list\n",
        "            A list of key points detected in each frame.\n",
        "        \"\"\"\n",
        "        output_width = 640  # Width of the resized frame for model input\n",
        "        output_height = 360  # Height of the resized frame for model input\n",
        "        scale = 2  # Scale factor to adjust the key points to the original image size\n",
        "\n",
        "        kps_res = []  # List to store the detected key points for each frame\n",
        "        matrixes_res = []  # List to store the transformation matrices for each frame\n",
        "\n",
        "        # Process each frame in the input sequence\n",
        "        for num_frame, image in enumerate(tqdm(frames)):\n",
        "            # Resize the frame to the specified input size\n",
        "            img = cv2.resize(image, (output_width, output_height))\n",
        "\n",
        "            # Normalize the image for the model input\n",
        "            inp = (img.astype(np.float32) / 255.)\n",
        "            inp = torch.tensor(np.rollaxis(inp, 2, 0))  # Change image axis order to (channels, height, width)\n",
        "            inp = inp.unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "            # Perform inference with the neural network\n",
        "            out = self.model(inp.float().to(self.device))[0]\n",
        "            pred = F.sigmoid(out).detach().cpu().numpy()  # Apply sigmoid activation and convert to numpy\n",
        "\n",
        "            points = []  # List to store the detected key points for the current frame\n",
        "\n",
        "            # Detect key points from heatmaps\n",
        "            for kps_num in range(14):\n",
        "                heatmap = (pred[kps_num] * 255).astype(np.uint8)  # Convert the heatmap to an 8-bit image\n",
        "                ret, heatmap = cv2.threshold(heatmap, 170, 255, cv2.THRESH_BINARY)  # Apply threshold to binarize\n",
        "\n",
        "                # Use Hough Circles to detect circular key points in the heatmap\n",
        "                circles = cv2.HoughCircles(heatmap, cv2.HOUGH_GRADIENT, dp=1, minDist=20, param1=50, param2=2,\n",
        "                                           minRadius=10, maxRadius=25)\n",
        "                if circles is not None:\n",
        "                    # Calculate the predicted key point location in the original image scale\n",
        "                    x_pred = circles[0][0][0] * scale\n",
        "                    y_pred = circles[0][0][1] * scale\n",
        "\n",
        "                    # Refine the key point location for certain key points\n",
        "                    if kps_num not in [8, 12, 9]:\n",
        "                        x_pred, y_pred = refine_kps(image, int(y_pred), int(x_pred), crop_size=40)\n",
        "\n",
        "                    points.append((x_pred, y_pred))  # Append the detected key point\n",
        "                else:\n",
        "                    points.append(None)  # Append None if no key point is detected\n",
        "\n",
        "            # Estimate the transformation matrix using the detected key points\n",
        "            matrix_trans = get_trans_matrix(points)\n",
        "            points = None\n",
        "\n",
        "            # If a valid transformation matrix is found, apply perspective transformation to key points\n",
        "            if matrix_trans is not None:\n",
        "                points = cv2.perspectiveTransform(refer_kps, matrix_trans)\n",
        "                matrix_trans = cv2.invert(matrix_trans)[1]\n",
        "\n",
        "            # Append the results (key points and transformation matrix) for the current frame\n",
        "            kps_res.append(points)\n",
        "            matrixes_res.append(matrix_trans)\n",
        "\n",
        "        return matrixes_res, kps_res  # Return the transformation matrices and key points for all frames\n"
      ],
      "metadata": {
        "id": "OUeejz5tDcMp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "09PuE-haSR1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilities"
      ],
      "metadata": {
        "id": "LuU0MQyyMQP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install the PySceneDetect library for scene detection in videos\n",
        "!pip install scenedetect"
      ],
      "metadata": {
        "id": "OWKM6_9yOFE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba12c6d1-86ba-4e96-f3f0-fc081a3ba727"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scenedetect in /usr/local/lib/python3.10/dist-packages (0.6.4)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from scenedetect) (8.1.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scenedetect) (1.26.4)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from scenedetect) (4.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scenedetect) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scenedetect.video_manager import VideoManager\n",
        "from scenedetect.scene_manager import SceneManager\n",
        "from scenedetect.stats_manager import StatsManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "\n",
        "def scene_detect(path_video):\n",
        "    \"\"\"\n",
        "    Detect and split the video into disjoint fragments based on scene changes using color histograms.\n",
        "\n",
        "    This function uses PySceneDetect to analyze the video and detects scene boundaries based on content changes.\n",
        "    It then returns the frame numbers for each detected scene.\n",
        "\n",
        "    Parameters:\n",
        "    path_video : str\n",
        "        Path to the input video file.\n",
        "\n",
        "    Returns:\n",
        "    scenes : list of lists\n",
        "        A list where each element is a list containing the start and end frame numbers for each detected scene.\n",
        "    \"\"\"\n",
        "    # Create a VideoManager object to handle the video file\n",
        "    video_manager = VideoManager([path_video])\n",
        "\n",
        "    # Initialize StatsManager to store statistics of the video (e.g., scene list, content values)\n",
        "    stats_manager = StatsManager()\n",
        "\n",
        "    # Initialize SceneManager to manage scene detection logic\n",
        "    scene_manager = SceneManager(stats_manager)\n",
        "\n",
        "    # Add a content-based scene detector with a custom threshold\n",
        "    scene_manager.add_detector(ContentDetector(threshold=30.0))  # Adjust threshold as needed\n",
        "\n",
        "    # Get the base timecode of the video (used to track frame positions)\n",
        "    base_timecode = video_manager.get_base_timecode()\n",
        "\n",
        "    # Downscale the video for faster processing if needed\n",
        "    video_manager.set_downscale_factor(2)  # Adjust downscale factor as needed\n",
        "\n",
        "    # Start video manager and load video frames for analysis\n",
        "    video_manager.start()\n",
        "\n",
        "    # Perform scene detection by analyzing video frames\n",
        "    scene_manager.detect_scenes(frame_source=video_manager)\n",
        "\n",
        "    # Retrieve the list of detected scenes, with each scene defined by start and end timecodes\n",
        "    scene_list = scene_manager.get_scene_list(base_timecode)\n",
        "\n",
        "    # If no scenes were detected, return the whole video as a single scene\n",
        "    if not scene_list:\n",
        "        scene_list = [(video_manager.get_base_timecode(), video_manager.get_current_timecode())]\n",
        "\n",
        "    # Convert the scene list to frame numbers (start and end frames) and return it\n",
        "    scenes = [[x[0].frame_num, x[1].frame_num] for x in scene_list]\n",
        "\n",
        "    # Release resources\n",
        "    video_manager.release()\n",
        "\n",
        "    return scenes\n"
      ],
      "metadata": {
        "id": "ufuS2bA0Crlu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Score Detection**"
      ],
      "metadata": {
        "id": "voyUqN-yVY81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScoreTracker:\n",
        "    def __init__(self):\n",
        "        self.player_top_points = 0\n",
        "        self.player_bottom_points = 0\n",
        "        self.top_side = \"top\"\n",
        "        self.bottom_side = \"bottom\"\n",
        "\n",
        "    def update_score(self, last_bounce_side, current_bounce_side, ball_out_of_bounds):\n",
        "        if ball_out_of_bounds:\n",
        "            # If ball is out of bounds, the player on the opposite side wins the point\n",
        "            if last_bounce_side == self.top_side:\n",
        "                self.player_bottom_points += 1\n",
        "            else:\n",
        "                self.player_top_points += 1\n",
        "        elif last_bounce_side == current_bounce_side:\n",
        "            # If the ball bounces twice on the same side, the opponent wins the point\n",
        "            if current_bounce_side == self.top_side:\n",
        "                self.player_bottom_points += 1\n",
        "            else:\n",
        "                self.player_top_points += 1\n",
        "\n",
        "    def get_score(self):\n",
        "        return f\"Player 1 Points: {self.player_top_points}, Player 2 Points: {self.player_bottom_points}\"\n",
        "\n",
        "def check_if_ball_is_out(ball_point, court_boundaries):\n",
        "    \"\"\"\n",
        "    Check if the ball is out of the court boundaries.\n",
        "    ball_point: (x, y) position of the ball.\n",
        "    court_boundaries: List of court points after homography transformation.\n",
        "    \"\"\"\n",
        "    x, y = ball_point\n",
        "    left_bound = min(court_boundaries, key=lambda p: p[0])[0]\n",
        "    right_bound = max(court_boundaries, key=lambda p: p[0])[0]\n",
        "    top_bound = min(court_boundaries, key=lambda p: p[1])[1]\n",
        "    bottom_bound = max(court_boundaries, key=lambda p: p[1])[1]\n",
        "\n",
        "    # Check if the ball is inside or outside court bounds\n",
        "    return not (left_bound <= x <= right_bound and top_bound <= y <= bottom_bound)\n",
        "\n",
        "def get_bounce_side(ball_point, court_midline):\n",
        "    \"\"\"\n",
        "    Determine which side of the court the ball bounced on based on the y-coordinate.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compare y-coordinate with court midline\n",
        "    if ball_point[1] < court_midline:\n",
        "        return \"top\"\n",
        "    else:\n",
        "        return \"bottom\""
      ],
      "metadata": {
        "id": "D2rGnXLyIMNO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UpT-5NqASVN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!ffmpeg -i input_videooo.mp4 -vf scale=480:-1 output_video_resized.mp4\n"
      ],
      "metadata": {
        "id": "3_u4MtxKCVXh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final : **Main Function**( also containes functions for **speed** and **distance** calulation for generating player metrics)"
      ],
      "metadata": {
        "id": "pRRU3eF3T7uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from court_detection_net import CourtDetectorNet\n",
        "\n",
        "#from court_reference import CourtReference\n",
        "#from bounce_detector import BounceDetector\n",
        "#from person_detector import PersonDetector\n",
        "#from ball_detector import BallDetector\n",
        "#from utils import scene_detect\n",
        "import argparse\n",
        "\n",
        "def read_video(path_video):\n",
        "    cap = cv2.VideoCapture(path_video)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frames = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frames.append(frame)\n",
        "        else:\n",
        "            break\n",
        "    cap.release()\n",
        "    return frames, fps\n",
        "\n",
        "\n",
        "\n",
        "def get_court_img():\n",
        "     court_reference = CourtReference()\n",
        "     court = court_reference.build_court_reference()\n",
        "     court = cv2.dilate(court, np.ones((10, 10), dtype=np.uint8))\n",
        "     court_img = (np.stack((court, court, court), axis=2)*255).astype(np.uint8)\n",
        "     return court_img\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def main(frames, scenes, bounces, ball_track, homography_matrices, kps_court, persons_top, persons_bottom, fps,\n",
        "         draw_trace=False, trace=7):\n",
        "    \"\"\"\n",
        "    :params\n",
        "        frames: list of original images\n",
        "        scenes: list of beginning and ending of video fragment\n",
        "        bounces: list of image numbers where the ball touches the ground\n",
        "        ball_track: list of (x,y) ball coordinates\n",
        "        homography_matrices: list of homography matrices\n",
        "        kps_court: list of 14 key points of tennis court\n",
        "        persons_top: list of person bboxes located in the top of tennis court\n",
        "        persons_bottom: list of person bboxes located in the bottom of tennis court\n",
        "        fps: frames per second\n",
        "        draw_trace: whether to draw ball trace\n",
        "        trace: the length of ball trace\n",
        "    :return\n",
        "        imgs_res: list of resulting images\n",
        "    \"\"\"\n",
        "    imgs_res = []\n",
        "    width_minimap = 166\n",
        "    height_minimap = 350\n",
        "    is_track = [x is not None for x in homography_matrices]\n",
        "\n",
        "    # Initialize score tracker\n",
        "    score_tracker = ScoreTracker()\n",
        "    court_ref = CourtReference()\n",
        "\n",
        "    # Estimate court midline from keypoints\n",
        "    court_midline = court_ref.net[0][1]\n",
        "\n",
        "    # Initialize variables\n",
        "    last_bounce_side = None\n",
        "    rally_lengths = []  # To store the lengths of each rally\n",
        "    current_rally_length = 0  # To track the current rally length\n",
        "    rally_count = 0  # To count the number of rallies\n",
        "\n",
        "    # Loop over scenes\n",
        "    for num_scene in range(len(scenes)):\n",
        "        sum_track = sum(is_track[scenes[num_scene][0]:scenes[num_scene][1]])\n",
        "        len_track = scenes[num_scene][1] - scenes[num_scene][0]\n",
        "\n",
        "        eps = 1e-15\n",
        "        scene_rate = sum_track / (len_track + eps)\n",
        "\n",
        "        if scene_rate > 0.5:\n",
        "            court_img = get_court_img()\n",
        "\n",
        "            # Variables to store distances and active times\n",
        "            distance_top, distance_bottom = 0, 0\n",
        "            active_time_top, active_time_bottom = 0, 0\n",
        "\n",
        "            # Loop over frames in the current scene\n",
        "            for i in range(scenes[num_scene][0], scenes[num_scene][1]):\n",
        "                img_res = frames[i].copy()  # Copy the current frame for processing\n",
        "                inv_mat = homography_matrices[i]\n",
        "\n",
        "                # Draw ball trajectory\n",
        "                if ball_track[i][0]:\n",
        "                    if draw_trace:\n",
        "                        for j in range(trace):\n",
        "                            if i - j >= 0 and ball_track[i - j][0]:\n",
        "                                draw_x = int(ball_track[i - j][0])\n",
        "                                draw_y = int(ball_track[i - j][1])\n",
        "                                img_res = cv2.circle(img_res, (draw_x, draw_y), radius=3, color=(0, 255, 0), thickness=2)\n",
        "                    else:\n",
        "                        img_res = cv2.circle(img_res, (int(ball_track[i][0]), int(ball_track[i][1])), radius=5, color=(0, 255, 0), thickness=2)\n",
        "                        img_res = cv2.putText(img_res, 'ball', org=(int(ball_track[i][0]) + 8, int(ball_track[i][1]) + 8),\n",
        "                                              fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.8, thickness=2, color=(0, 255, 0))\n",
        "\n",
        "                # Draw court keypoints\n",
        "                if kps_court[i] is not None:\n",
        "                    for j in range(len(kps_court[i])):\n",
        "                        img_res = cv2.circle(img_res, (int(kps_court[i][j][0, 0]), int(kps_court[i][j][0, 1])), radius=0, color=(0, 0, 255), thickness=10)\n",
        "\n",
        "                height, width, _ = img_res.shape\n",
        "\n",
        "                # Draw bounce in minimap\n",
        "                # Handle ball bounces\n",
        "                if i in bounces and inv_mat is not None:\n",
        "                    ball_point = ball_track[i]\n",
        "                    ball_point = np.array(ball_point, dtype=np.float32).reshape(1, 1, 2)\n",
        "                    ball_point = cv2.perspectiveTransform(ball_point, inv_mat)\n",
        "\n",
        "                    # Draw bounce on the minimap\n",
        "                    court_img = cv2.circle(court_img, (int(ball_point[0, 0, 0]), int(ball_point[0, 0, 1])),\n",
        "                                           radius=0, color=(0, 255, 255), thickness=50)\n",
        "\n",
        "                    # Check if ball is out of bounds\n",
        "                    court_boundaries = np.array(court_ref.key_points, dtype=np.float32).squeeze()\n",
        "                    ball_out_of_bounds = check_if_ball_is_out(ball_point.squeeze(), court_boundaries)\n",
        "\n",
        "                    # Determine which side the ball bounced on\n",
        "                    current_bounce_side = get_bounce_side(ball_point.squeeze(), court_midline)\n",
        "\n",
        "                    # Update the score based on bounce information\n",
        "                    score_tracker.update_score(last_bounce_side, current_bounce_side, ball_out_of_bounds)\n",
        "\n",
        "                    # Update rally length\n",
        "                    current_rally_length += 1\n",
        "\n",
        "                    # If the ball is out, or a new rally starts, store and reset the rally length\n",
        "                    if ball_out_of_bounds or (last_bounce_side==current_bounce_side):\n",
        "                        rally_lengths.append(current_rally_length)\n",
        "                        rally_count += 1\n",
        "                        current_rally_length = 0  # Reset for the new rally\n",
        "\n",
        "                    # Update last bounce side\n",
        "                    last_bounce_side = current_bounce_side\n",
        "\n",
        "                # Display current rally length\n",
        "                img_res = cv2.putText(img_res, f'Rally Length: {current_rally_length}', (50, 100),\n",
        "                                      cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "                minimap = court_img.copy()\n",
        "\n",
        "                # Draw persons and calculate distances\n",
        "                persons = persons_top[i] + persons_bottom[i]\n",
        "                for j,person in enumerate(persons):\n",
        "                    if len(person[0]) > 0:\n",
        "                        person_bbox = list(person[0])\n",
        "                        img_res = cv2.rectangle(img_res, (int(person_bbox[0]), int(person_bbox[1])),\n",
        "                                                (int(person_bbox[2]), int(person_bbox[3])), [255, 0, 0], 2)\n",
        "                        img_res = cv2.putText(img_res, f\"Player ID: {j+1}\",\n",
        "                                              (int(person_bbox[0]), int(person_bbox[1] - 10)),\n",
        "                                              cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "\n",
        "                        # Transform the foot position to the top-down view using perspectiveTransform\n",
        "                        person_point = list(person[1])\n",
        "                        person_point = np.array(person_point, dtype=np.float32).reshape(1, 1, 2)\n",
        "                        person_point = cv2.perspectiveTransform(person_point, inv_mat)\n",
        "                        minimap = cv2.circle(minimap, (int(person_point[0, 0, 0]), int(person_point[0, 0, 1])),\n",
        "                                             radius=0, color=(255, 0, 0), thickness=80)\n",
        "\n",
        "                 # Calculate distances and speeds for both players\n",
        "                if i > 0:  # Avoid calculation on the first frame\n",
        "                    # Top player distance and speed\n",
        "                    if len(persons_top[i]) > 0 and len(persons_top[i - 1]) > 0:\n",
        "                        prev_top = np.array(persons_top[i - 1][0][1], dtype=np.float32).reshape(1, 1, 2)\n",
        "                        curr_top = np.array(persons_top[i][0][1], dtype=np.float32).reshape(1, 1, 2)\n",
        "                        prev_position_top = cv2.perspectiveTransform(prev_top, inv_mat)[0][0]\n",
        "                        curr_position_top = cv2.perspectiveTransform(curr_top, inv_mat)[0][0]\n",
        "                        distance_top += (np.linalg.norm(np.array(prev_position_top) - np.array(curr_position_top)))/101.27\n",
        "                        speed_top= ((np.linalg.norm(np.array(prev_position_top) - np.array(curr_position_top)))/101.27)*fps\n",
        "                        active_time_top += 1/fps\n",
        "\n",
        "                    # Bottom player distance and speed\n",
        "                    if len(persons_bottom[i]) > 0 and len(persons_bottom[i - 1]) > 0:\n",
        "                        prev_bottom = np.array(persons_bottom[i - 1][0][1], dtype=np.float32).reshape(1, 1, 2)\n",
        "                        curr_bottom = np.array(persons_bottom[i][0][1], dtype=np.float32).reshape(1, 1, 2)\n",
        "                        prev_position_bottom = cv2.perspectiveTransform(prev_bottom, inv_mat)[0][0]\n",
        "                        curr_position_bottom = cv2.perspectiveTransform(curr_bottom, inv_mat)[0][0]\n",
        "                        distance_bottom += (np.linalg.norm(np.array(prev_position_bottom) - np.array(curr_position_bottom)))/101.27\n",
        "                        speed_bottom= ((np.linalg.norm(np.array(prev_position_bottom) - np.array(curr_position_bottom)))/101.27)*fps\n",
        "                        active_time_bottom += 1/fps\n",
        "\n",
        "                # Display player metrics\n",
        "                if active_time_top*fps > 1 and active_time_bottom*fps > 1:\n",
        "                    img_res = cv2.putText(img_res, f'Player 1: Distance: {distance_top:.2f} m, Active Time: {round(active_time_top,2)} s, Speed: {speed_top:.2f} m/s',\n",
        "                                          org=(690, 600), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.4, color=(255, 255, 255), thickness=2)\n",
        "                    img_res = cv2.putText(img_res, f'Player 2: Distance: {distance_bottom:.2f} m, Active Time: {round(active_time_bottom,2)} s, Speed: {speed_bottom:.2f} m/s',\n",
        "                                          org=(690, 620), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.4, color=(255, 255, 255), thickness=2)\n",
        "\n",
        "\n",
        "                # Add minimap to result image\n",
        "                minimap = cv2.resize(minimap, (width_minimap, height_minimap))\n",
        "                img_res[30:(30 + height_minimap), (width - 30 - width_minimap):(width - 30), :] = minimap\n",
        "\n",
        "                # Store the resulting frame\n",
        "                imgs_res.append(img_res)\n",
        "                score_text = score_tracker.get_score()\n",
        "                img_res = cv2.putText(img_res, score_text, (50, 50),\n",
        "                                      cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "        else:\n",
        "            imgs_res.extend(frames[scenes[num_scene][0]:scenes[num_scene][1]])\n",
        "    # Final score output\n",
        "    final_score = score_tracker.get_score()\n",
        "    print(f\"Final Score: {final_score}\")\n",
        "\n",
        "    # Print rally information\n",
        "    print(f\"Total number of rallies: {rally_count}\")\n",
        "    for idx, rally_length in enumerate(rally_lengths, 1):\n",
        "        print(f\"Rally {idx}: {rally_length} shots\")\n",
        "\n",
        "    return imgs_res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def write(imgs_res, fps, path_output_video):\n",
        "    height, width = imgs_res[0].shape[:2]\n",
        "    out = cv2.VideoWriter(path_output_video, cv2.VideoWriter_fourcc(*'DIVX'), fps, (width, height))\n",
        "    for num in range(len(imgs_res)):\n",
        "        frame = imgs_res[num]\n",
        "        out.write(frame)\n",
        "    out.release()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Check if running in a Jupyter notebook or a script\n",
        "    try:\n",
        "        parser = argparse.ArgumentParser()\n",
        "        parser.add_argument('--path_ball_track_model', type=str, help='path to pretrained model for ball detection')\n",
        "        parser.add_argument('--path_court_model', type=str, help='path to pretrained model for court detection')\n",
        "        parser.add_argument('--path_bounce_model', type=str, help='path to pretrained model for bounce detection')\n",
        "        parser.add_argument('--path_input_video', type=str, help='path to input video')\n",
        "        parser.add_argument('--path_output_video', type=str, help='path to output video')\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    except SystemExit:\n",
        "        # If run in a Jupyter notebook, set the arguments manually\n",
        "        args = argparse.Namespace(\n",
        "            path_ball_track_model='/content/model_best.pt',  # replace with the actual path\n",
        "            path_court_model='/content/model_tennis_court_det.pt',  # replace with the actual path\n",
        "            path_bounce_model='/content/ctb_regr_bounce.cbm',  # replace with the actual path\n",
        "            path_input_video='/content/input_video.mp4',  # replace with the actual video path\n",
        "            path_output_video='path_to_output_video.mp4'  # replace with the desired output path\n",
        "        )\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    frames, fps = read_video(args.path_input_video)\n",
        "    scenes = scene_detect(args.path_input_video)\n",
        "\n",
        "    print('ball detection')\n",
        "    ball_detector = BallDetector(args.path_ball_track_model, device)\n",
        "    ball_track = ball_detector.infer_model(frames)\n",
        "\n",
        "    print('court detection')\n",
        "    court_detector = CourtDetectorNet(args.path_court_model, device)\n",
        "    homography_matrices, kps_court = court_detector.infer_model(frames)\n",
        "\n",
        "    print('person detection')\n",
        "    person_detector = PersonDetector(device)\n",
        "    persons_top, persons_bottom = person_detector.track_players(frames, homography_matrices, filter_players=False)\n",
        "\n",
        "    # bounce detection\n",
        "    bounce_detector = BounceDetector(args.path_bounce_model)\n",
        "    x_ball = [x[0] for x in ball_track]\n",
        "    y_ball = [x[1] for x in ball_track]\n",
        "    bounces = bounce_detector.predict(x_ball, y_ball)\n",
        "\n",
        "    imgs_res = main(frames, scenes, bounces, ball_track, homography_matrices, kps_court, persons_top, persons_bottom, fps,\n",
        "                    draw_trace=True)\n",
        "\n",
        "    write(imgs_res, fps, args.path_output_video)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWMEywC5ErFY",
        "outputId": "12ac0baf-6743-48b7-99bb-4146ea59f9b0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--path_ball_track_model PATH_BALL_TRACK_MODEL]\n",
            "                                [--path_court_model PATH_COURT_MODEL]\n",
            "                                [--path_bounce_model PATH_BOUNCE_MODEL]\n",
            "                                [--path_input_video PATH_INPUT_VIDEO]\n",
            "                                [--path_output_video PATH_OUTPUT_VIDEO]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-1a6d0663-abfe-4676-99fb-8216f95edb7a.json\n",
            "ERROR:pyscenedetect:VideoManager is deprecated and will be removed.\n",
            "INFO:pyscenedetect:Loaded 1 video, framerate: 30.000 FPS, resolution: 1280 x 720\n",
            "INFO:pyscenedetect:Downscale factor set to 5, effective resolution: 256 x 144\n",
            "INFO:pyscenedetect:Detecting scenes...\n",
            "ERROR:pyscenedetect:`base_timecode` argument is deprecated and has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ball detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 212/212 [00:24<00:00,  8.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "court detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 214/214 [00:28<00:00,  7.63it/s]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "person detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 214/214 [00:34<00:00,  6.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Score: Player 1 Points: 0, Player 2 Points: 0\n",
            "Total number of rallies: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "G1OtCSMUVqCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thankyou<3"
      ],
      "metadata": {
        "id": "SZaOar69VlYW"
      }
    }
  ]
}